---
output:
  pdf_document: default
  html_document: default
---
```{r}
#clear global environment
rm(list=ls())
```

```{r}
install.packages("doParallel", "foreach")
install.packages("progressr")
```





```{r}
#define simulated dataset
# Y=a+bx+eps
set.seed(4)
n<-100
x <- rnorm(n, mean = 5, sd = 2)
  alpha_true <- 2                  # True intercept
  beta_true <- 3                   # True slope
  sigma <- 5  
  #add white noise to linear relationship
Y <- alpha_true + beta_true * x+rnorm(n, sd = sigma)
X<-matrix(c(rep(1,n),x), nrow=n,ncol=2)
dim(as.matrix(Y))
dim(X)
```


```{r}
bayes.SLR.gibbs <- function(n = 100, alpha_prior_mean = 0, alpha_prior_sd = 1, 
                           beta_prior_mean = 0, beta_prior_sd = 1, 
                           iter = 8000, seed = 4, burnin=4000) {
  
  set.seed(seed)  # Set seed for reproducibility
  
  # Simulate data
  x <- rnorm(n, mean = 5, sd = 2)  # Predictor variable
  alpha_true <- 2                  # True intercept
  beta_true <- 3                   # True slope
  sigma <- 5                        # Known standard deviation
  y <- alpha_true + beta_true * x + rnorm(n, sd = sigma)  # Response variable

  
  # Initialize storage for samples
  alpha_samples <- rep(NA, iter)
  beta_samples <- rep(NA, iter)
  
  # Initial values for alpha and beta
  alpha <- rnorm(1, mean=0, sd=1)
  beta <- rnorm(1, mean=0, sd=1)
  
  # Gibbs sampling
  for (i in 1:iter+burnin) {
    # Update alpha
    alpha_posterior_var <- 1 / (1 / alpha_prior_sd^2 + n / sigma^2)
    alpha_posterior_mean <- alpha_posterior_var * (
      alpha_prior_mean / alpha_prior_sd^2 + sum(y - beta * x) / sigma^2
    )
    alpha <- rnorm(1, mean = alpha_posterior_mean, sd = sqrt(alpha_posterior_var))
    
    # Update beta
    beta_posterior_var <- 1 / (1 / beta_prior_sd^2 + sum(x^2) / sigma^2)
    beta_posterior_mean <- beta_posterior_var * (
      beta_prior_mean / beta_prior_sd^2 + sum((y - alpha) * x) / sigma^2
    )
    beta <- rnorm(1, mean = beta_posterior_mean, sd = sqrt(beta_posterior_var))
    
    # Store samples
    alpha_samples[i] <- alpha
    beta_samples[i] <- beta
  }
  
  # Summarize posterior samples
  alpha_samples<-alpha_samples[-c(1:burnin)]
  beta_samples<-beta_samples[-c(1:burnin)]
  alpha_mean <- mean(alpha_samples)
  beta_mean <- mean(beta_samples)
    return(invisible(list(alpha_samples = alpha_samples, 
                        beta_samples = beta_samples, 
                        alpha_mean = alpha_mean, 
                        beta_mean = beta_mean)))
}
```


```{r}
eps<-0.03
#standard normal prior
well.spec<-bayes.SLR.gibbs(iter=8000)
#miss-specified prior
miss.spec<-bayes.SLR.gibbs(alpha_prior_mean = 0, alpha_prior_sd = sqrt(eps), 
                                beta_prior_mean = 0, beta_prior_sd = sqrt(eps), iter=8000)
well.spec$alpha_mean
well.spec$beta_mean
miss.spec$alpha_mean
miss.spec$beta_mean
```




```{r}
#function to calculate parameters for analytical posterior distribution
# inputs are prior mean and prior covariance matrix 
posterior.params <- function(pri.mu,pri.sigma, seed=4, n=100){
  set.seed(seed)
x <- rnorm(n, mean = 5, sd = 2)
  alpha_true <- 2                  # True intercept
  beta_true <- 3                   # True slope
  sigma <- 5   
Y <- alpha_true + beta_true * x+rnorm(n, sd = sigma)
X<-matrix(c(rep(1,n),x), nrow=n,ncol=2)
z<-(1/sigma^2)*t(X)%*%X+solve(pri.sigma)
theta_mu<-solve(z)%*%(solve(pri.sigma)%*%pri.mu + (1/sigma^2)*t(X)%*%Y)
list(theta_mu=theta_mu, theta_var_mat=solve(z))
  }
```


```{r}
#function to calculate parameters for analytical posterior distribution
# inputs are prior mean and prior covariance matrix 
posterior.params.v2 <- function(pri.mu,pri.sigma, seed=4, n=100){
  set.seed(seed)
x <- rnorm(n, mean = 5, sd = 2)
  alpha_true <- 2                  # True intercept
  beta_true <- 3                   # True slope
  sigma <- 5   
Y <- alpha_true + beta_true * x+rnorm(n, sd = sigma)
X<-matrix(c(rep(1,n),x), nrow=n,ncol=2)
A0 <- solve(pri.sigma)
An <- t(X)%*%X + A0
theta_mu<-solve(An)%*%(A0%*%pri.mu+t(X)%*%Y)
list(theta_mu=theta_mu, theta_var_mat=(sigma^2)*solve(An))
  }
```

```{r}
eps<-0.3
posterior.params(pri.mu = c(0,0), pri.sigma = eps*diag(2))
posterior.params.v2(pri.mu = c(0,0), pri.sigma = eps*diag(2))
```


```{r}
#create very small variance
well.spec.post.mu<-posterior.params(pri.mu = c(0,0), pri.sigma = 1*diag(2))$theta_mu 
well.spec.post.varmat<-posterior.params(pri.mu = c(0,0), pri.sigma = 1*diag(2))$theta_var_mat
well.spec.post.mu
well.spec.post.varmat
miss.spec.post.mu<-posterior.params(pri.mu = c(0,0), pri.sigma = eps*diag(2))$theta_mu 
miss.spec.post.varmat<-posterior.params(pri.mu = c(0,0), pri.sigma = eps*diag(2))$theta_var_mat
miss.spec.post.mu
miss.spec.post.varmat
```






```{r}
#Well Specified case
# Plot the posterior distributions
hist(well.spec$alpha_samples, breaks=100, main = "Posterior of alpha", xlab = "alpha", freq = F)
# Create a sequence of x values for the normal distribution curve
a_x_values <- seq(min(well.spec$alpha_samples), max(well.spec$alpha_samples), length = 100)

# Calculate the marginal density for those x values
alpha_normal_density <- dnorm(a_x_values, mean = well.spec.post.mu[1], sd = sqrt(well.spec.post.varmat[1,1]))

# Add the normal distribution density curve to the histogram
lines(a_x_values, alpha_normal_density, col = "red", lwd = 2)
# Plot the posterior distributions
hist(well.spec$beta_samples, breaks=100, main = "Posterior of beta", xlab = "beta", freq = F)
# Create a sequence of x values for the normal distribution curve
b_x_values <- seq(min(well.spec$beta_samples), max(well.spec$beta_samples), length = 100)

# Calculate marginal density for those x values
beta_normal_density <- dnorm(b_x_values, mean = well.spec.post.mu[2], sd = sqrt(well.spec.post.varmat[2,2]))

# Add the normal distribution density curve to the histogram
lines(b_x_values, beta_normal_density, col = "blue", lwd = 2)
```
```{r}
#Miss Specified case
# Plot the posterior distributions
hist(miss.spec$alpha_samples, breaks=100, main = "Posterior of alpha(miss-specified)", xlab = "alpha", freq = F, xlim = c(-3.5,3.5))
# Create a sequence of x values for the normal distribution curve
a_x_values <- seq(-3.5, 3.5, length = 1000)

# Calculate the marginal density for those x values
alpha_normal_density <- dnorm(a_x_values, mean = miss.spec.post.mu[1], sd = sqrt(miss.spec.post.varmat[1,1]))

# Add the normal distribution density curve to the histogram
lines(a_x_values, alpha_normal_density, col = "red", lwd = 2)
# Add a vertical line for the true alpha value
abline(v = 2, col = "black", lwd = 2, lty = 2)  # True alpha value
# Plot the posterior distributions
hist(miss.spec$beta_samples, breaks=50, main = "Posterior of beta(miss-specified)", xlab = "beta", freq = F, xlim=c(1,3))
# Create a sequence of x values for the normal distribution curve
b_x_values <- seq(1, 3, length = 1000)

# Calculate marginal density for those x values
beta_normal_density <- dnorm(b_x_values, mean = miss.spec.post.mu[2], sd = sqrt(miss.spec.post.varmat[2,2]))

# Add the normal distribution density curve to the histogram
lines(b_x_values, beta_normal_density, col = "blue", lwd = 2)
# Add a vertical line for the true beta value
abline(v = 3, col = "black", lwd = 2, lty = 2)  # True beta value
```



```{r}
#put parameters into single vector
well.theta<-matrix(c(well.spec$alpha_samples,well.spec$beta_samples), nrow=2, byrow = T)
miss.theta<-matrix(c(miss.spec$alpha_samples,miss.spec$beta_samples), nrow=2, byrow = T)

```




```{r}
posterior_predictive<- function(Y, X, y, x_new, theta, sigma) {
  #Y and X are univariate evaluation points
  #y and x_new are  a reference point that determine the weights
  #theta is MCMC parameter samples
  #sigma is known model parameter
  # Compute all weights in log scale
  log_w <- sapply(1:ncol(theta), function(t) {
    dnorm(y, mean = x_new %*% theta[, t], sd = sigma, log=T)
  })
  #log-sum-exp for numerical stability
  max_log_w <- max(log_w)
log_sum_exp <- max_log_w + log(sum(exp(log_w - max_log_w)))
normalized_weights <- exp(log_w - log_sum_exp)
  
  # Compute weighted likelihoods
  weighted_likelihoods <- sapply(1:ncol(theta), function(t) {
    normalized_weights[t] * dnorm(Y, mean = X %*% theta[, t], sd = sigma)
  })
  
  # Return the sum of weighted likelihoods
sum(weighted_likelihoods)
}

```

```{r}
posterior_predictive.v2 <- function(Y, X, y, x_new, theta, sigma) {
  # Compute weights
  log_w <- apply(theta, 2, function(th) dnorm(y, mean = x_new %*% th, sd = sigma, log=TRUE))
  max_log_w <- max(log_w)
  log_sum_exp <- max_log_w + log(sum(exp(log_w - max_log_w)))
  normalized_weights <- exp(log_w - log_sum_exp)
  
  # Get dimensions
  n_eval <- length(Y)
  n_samples <- ncol(theta)
  
  # Calculate all means for all evaluation points and samples
  all_means <- X %*% theta
  
  # Create an array that repeats Y for each sample
  # Create a n_eval Ã— n_samples matrix where each row is filled with one Y value
  Y_mat <- matrix(Y, nrow=n_eval, ncol=n_samples)
  
  # Calculate all densities at once
  all_densities <- dnorm(Y_mat, mean=all_means, sd=sigma)
  
  # Apply weights and sum rows
  result <- rowSums(all_densities * matrix(normalized_weights, 
                                           nrow=n_eval, 
                                           ncol=n_samples, 
                                           byrow=TRUE))
  
  return(result)
}
```






```{r}
length(Y[c(1:3)])
ny <-100
nx<- 50
y_grid <- seq(-5,44, length.out = ny)
x_grid <- seq(min(x), max(x), length.out = nx )
x_new_mat<-matrix(c(rep(1,nx),x_grid ), nrow=nx)
posterior_predictive(Y = Y[1], X = X[1, , drop = FALSE], y = y_grid[1], x_new =x_new_mat[1, , drop=F], theta = miss.theta, sigma = sigma)
posterior_predictive(Y = Y[2], X = X[2, , drop = FALSE], y = y_grid[1], x_new =x_new_mat[1, , drop=F], theta = miss.theta, sigma = sigma)
posterior_predictive(Y = Y[3], X = X[3, , drop = FALSE], y = y_grid[1], x_new =x_new_mat[1, , drop=F], theta = miss.theta, sigma = sigma)
posterior_predictive.v2(Y = Y[c(1:3)], X = X[c(1:3), , drop = FALSE], y = y_grid[1], x_new =x_new_mat[1, , drop=F], theta = miss.theta, sigma = sigma)
```





```{r}
library(doParallel)
library(foreach)

parallel_full_conformal_prediction <- function(Y, X, y_grid, x_new, theta, sigma, alpha) {
  # Register parallel backend
  num_cores <- detectCores() - 1  # Use available cores minus one for safety
  cl <- makeCluster(num_cores)
  registerDoParallel(cl)
  
    # Export required functions and variables
  clusterExport(cl, varlist = c( "posterior_predictive", "Y", "X", "theta", "sigma", "alpha"), envir = environment())
  
  # Parallel execution using foreach
  accepted_y <- foreach(l = 1:length(y_grid), .combine = c, .packages = c("foreach")) %dopar% {
    # Compute conformity scores on dataset
    sig_1_to_n <- sapply(1:length(Y), function(i) 
      posterior_predictive(Y = Y[i], X = X[i, , drop = FALSE], y = y_grid[l], x_new = x_new, theta = theta, sigma = sigma)
    )

    # Conformity score on test point
    sig_n_plus_one <- posterior_predictive(Y = y_grid[l], X = x_new, y = y_grid[l], x_new = x_new, theta = theta, sigma = sigma)

    # Adjusted quantile calculation
    n <- length(Y)
    pi <- (length(which(sig_1_to_n <= sig_n_plus_one)) + 1) / (n + 1)

    # Only return y_grid[l] if it meets the condition
    if (pi > alpha) {
      y_grid[l]  # Return value (foreach will combine it)
    } else {
      NULL  # NULL values are ignored in .combine = c
    }
  }

  # Stop the cluster
  stopCluster(cl)
  
  return(accepted_y)
}

```


```{r}
full_conformal_prediction <- function(Y, X, y_grid, x_new, theta, sigma, alpha) {
  # x_new should be in the form c(1,x)
  accepted_y <- c()

  for (l in 1:length(y_grid)) {
    # Conformity scores on dataset
    sig_1_to_n <- sapply(1:length(Y), function(i) 
      posterior_predictive(Y = Y[i], X = X[i, , drop = FALSE], y = y_grid[l], x_new = x_new, theta = theta, sigma = sigma)
    )

    # Debugging: Print range of sig_1_to_n
   # message(paste0("y_grid[", l, "] = ", y_grid[l], 
                  # " | sig_1_to_n range: [", min(sig_1_to_n), ", ", max(sig_1_to_n), "]"))

    # Conformity score on test point
    sig_n_plus_one <- posterior_predictive(Y = y_grid[l], X = x_new, y = y_grid[l], x_new = x_new, theta = theta, sigma = sigma)

    # Debugging: Print sig_n_plus_one
    #message(paste0("sig_n_plus_one for y_grid[", l, "] = ", sig_n_plus_one))

    # Adjusted quantile calculation
    n <- length(Y)
    pi <- (length(which(sig_1_to_n <= sig_n_plus_one)) + 1) / (n + 1)

    # Reject points if pi <= alpha
    if (pi > alpha) {
      accepted_y <- c(accepted_y, y_grid[l])
    }
  }

  return(accepted_y)
}
```










```{r}
library(foreach)
library(doParallel)


conformal_bounds_parallel <- function(conf_x_new_mat, y_grid, theta) {
  num_cores <- detectCores() - 1  # Use available cores, keeping one free
  cl <- makeCluster(num_cores)
  registerDoParallel(cl)

  # Export required functions and variables
  clusterExport(cl, varlist = c("full_conformal_prediction", "posterior_predictive", "Y", "X", "theta", "sigma", "alpha"), envir = environment())



  # Parallel computation with progress updates
  conf_range <- foreach(i = 1:nrow(conf_x_new_mat), .combine = rbind, .packages = c("foreach")) %dopar% {
    x_new_i <- conf_x_new_mat[i, , drop = FALSE]  # Extract the current `x_new`
    
    # Compute conformal bounds
    bounds <- range(full_conformal_prediction(Y = Y, X = X, y_grid = y_grid, x_new = x_new_i, theta = theta, sigma = sigma, alpha = alpha))
  

    return(bounds)
  }

  stopCluster(cl)  # Ensure cluster shuts down properly

  return(conf_range)  # A matrix where each row is [lower, upper] for an `x_new`
}

```



```{r}
library(foreach)
library(doParallel)

conformal_bounds_single_parallel <- function(Y, X, conf_x_new_mat, y_grid, theta, sigma, alpha) {
  num_cores <- detectCores() - 1  # Use all but one core
  cl <- makeCluster(num_cores)
  registerDoParallel(cl)

  # Export required functions and variables
  clusterExport(cl, varlist = c("posterior_predictive.v2", "Y", "X", "y_grid", "theta", "sigma", "alpha"), envir = environment())

  conf_range <- foreach(i = 1:nrow(conf_x_new_mat), .combine = rbind, .packages = c("foreach")) %dopar% {
    x_new_i <- conf_x_new_mat[i, , drop = FALSE]  # Extract the current `x_new`

    # Compute conformal prediction set directly within the same loop
    accepted_y <- foreach(l = 1:length(y_grid), .combine = c) %do% {
      # Compute conformity scores on dataset
      sig_1_to_n <- posterior_predictive.v2(Y = Y, X = X, y = y_grid[l], x_new = x_new_i, theta = theta, sigma = sigma)

      # Conformity score on test point
      sig_n_plus_one <- posterior_predictive.v2(Y = y_grid[l], X = x_new_i, y = y_grid[l], x_new = x_new_i, theta = theta, sigma = sigma)

      # Adjusted quantile calculation
      n <- length(Y)
      pi <- (length(which(sig_1_to_n <= sig_n_plus_one)) + 1) / (n + 1)

      # Only return y_grid[l] if it meets the condition
      if (pi > alpha) {
        y_grid[l]
      } else {
        NULL
      }
    }

    # Compute lower and upper bounds
    bounds <- range(accepted_y)
    return(bounds)
  }

  stopCluster(cl)

  return(conf_range)  # Matrix where each row is [lower, upper] for an `x_new`
}

```



```{r}
library(foreach)
library(doParallel)

optimized_conformal_bounds <- function(Y, X, conf_x_new_mat, y_grid, theta, sigma, alpha) {
  num_cores <- detectCores() - 1  # Use all but one core
  cl <- makeCluster(num_cores)
  registerDoParallel(cl)

  # Export required functions and variables
  clusterExport(cl, varlist = c("posterior_predictive", "Y", "X", "y_grid", "theta", "sigma", "alpha"), envir = environment())

  conf_range <- foreach(i = 1:nrow(conf_x_new_mat), .combine = rbind, .packages = c("foreach")) %dopar% {
    x_new_i <- conf_x_new_mat[i, , drop = FALSE]  # Extract the current `x_new`

    # Process y_grid in a sequential loop (minimizing parallel overhead)
    accepted_y <- numeric(0)
    for (l in seq_along(y_grid)) {
      # Compute conformity scores on dataset
      sig_1_to_n <- sapply(seq_along(Y), function(j) 
        posterior_predictive(Y = Y[j], X = X[j, , drop = FALSE], y = y_grid[l], x_new = x_new_i, theta = theta, sigma = sigma)
      )

      # Conformity score on test point
      sig_n_plus_one <- posterior_predictive(Y = y_grid[l], X = x_new_i, y = y_grid[l], x_new = x_new_i, theta = theta, sigma = sigma)

      # Adjusted quantile calculation
      n <- length(Y)
      pi <- (length(which(sig_1_to_n <= sig_n_plus_one)) + 1) / (n + 1)

      # Only keep valid y_grid values
      if (pi > alpha) {
        accepted_y <- c(accepted_y, y_grid[l])
      }
    }

    # Compute lower and upper bounds
    bounds <- range(accepted_y)
    return(bounds)
  }

  stopCluster(cl)

  return(conf_range)  # Matrix where each row is [lower, upper] for an `x_new`
}

```



```{r}
library("doParallel")
library("foreach")
#create a y grid to loop over
#ny is grid fineness for y
ny <-150
nx<- 100
y_grid <- seq(-10,44, length.out = ny)
x_grid <- seq(min(x), max(x), length.out = nx )
#turn x-grid into design matrix
x_new_mat<-matrix(c(rep(1,nx),x_grid ), nrow=nx)
#this involves (nx)x4000x(100+ny)=16,000,000 likelihood evals
miss_conf_range <- conformal_bounds_single_parallel(Y, X, conf_x_new_mat=x_new_mat, y_grid, theta=miss.theta, sigma, alpha=0.1)


```



```{r}
miss_conf_range
```


```{r}
alpha<-0.1
# Create a fine grid of x values to approximate the credible bands
cred_x_grid <- seq(min(x), max(x), length.out = 100)
#create design matrix for x_new
cred_x_new_mat<-matrix(c(rep(1,100),cred_x_grid), nrow=100)
#Use quantiles to get credible intervals, using analytically derived density of the posterior predictive.
credible_lower <- function(x_new,mu,varmat) {
qnorm(alpha / 2, mean = x_new%*%mu, sd = sqrt(sigma^2+x_new%*%varmat%*%t(x_new)))
}

credible_upper <- function(x_new,mu,varmat) {
qnorm(1 - alpha / 2, mean = x_new%*%mu, sd = sqrt(sigma^2+x_new%*%varmat%*%t(x_new)))
}

# Calculate the credible interval bounds over the grid(drop preserves dimension of extracted vector)
cred.lower <- sapply(1:nrow(cred_x_new_mat), function(i) credible_lower(cred_x_new_mat[i, , drop=F], miss.spec.post.mu,
miss.spec.post.varmat))
cred.upper <- sapply(1:nrow(cred_x_new_mat), function(i) credible_upper(cred_x_new_mat[i, , drop=F], miss.spec.post.mu,
miss.spec.post.varmat))

#debugging
#credible_lower(cred_x_new_mat[100, , drop=F], miss.spec.post.mu,
#miss.spec.post.varmat)
```


```{r}
lm_fit <- lm(Y ~ X[, 2])
lm_fit
```


```{r}
# Extract lower and upper bounds
lower <- miss_conf_range[, 1]
upper <- miss_conf_range[, 2]

# Plot observed data
plot(X[, 2], Y, pch = 16, col = "black", xlab = "X values", ylab = "Y values",
     main = "Smooth Prediction Bands with Observed Data", ylim = c(-3,40))

# Fill the confidence region (blue)
polygon(c(x_grid, rev(x_grid)), c(lower, rev(upper)), col = rgb(0, 0, 1, 0.3), border = NA)

# Fill the region between red dotted lines
polygon(c(cred_x_grid, rev(cred_x_grid)), c(cred.lower, rev(cred.upper)), 
        col = rgb(1, 0, 0, 0.2), border = NA)

# Plot smooth upper and lower bounds
lines(x_grid, lower, col = "blue", lwd = 2)
lines(x_grid, upper, col = "blue", lwd = 2)
lines(cred_x_grid, cred.lower, col = "red", lwd = 2, lty = 2)  # Lower bound line
lines(cred_x_grid, cred.upper, col = "red", lwd = 2, lty = 2)  # Upper bound line

# Add the least squares line
lm_fit <- lm(Y ~ X[, 2])
abline(lm_fit, col = "darkgreen", lwd = 2)

# Optional: Add a legend
legend("topleft", 
       legend = c("Data", "Blue Conf. Band", "Red Conf. Band", "Least Squares Line"),
       pch = c(16, NA, NA, NA),
       lty = c(NA, 1, 2, 1),
       lwd = c(NA, 2, 2, 2),
       col = c("black", "blue", "red", "darkgreen"),
       fill = c(NA, rgb(0, 0, 1, 0.3), rgb(1, 0, 0, 0.2), NA),
       border = rep(NA, 4),
       bg = "white")
```

```{r}
```



```{r}
#create function to get upper and lower bound of predictive interval
conformal_bounds <- function(x_new,y_grid) {
  # Compute the range from full_conformal_prediction
  bounds <- range(full_conformal_prediction(Y = y, X = X, y_grid = y_grid, x_new = x_new, theta = theta, sigma = sigma, alpha = alpha))
  
  # Return the bounds as named values
  return(list(lower = bounds[1], upper = bounds[2]))
}
```








```{r}
alpha<-0.1
# Create a fine grid of x values to approximate the credible bands
cred_x_grid <- seq(min(x), max(x), length.out = 100)
#create design matrix for x_new
cred_x_new_mat<-matrix(c(rep(1,100),x_grid), nrow=100)
#Use quantiles to get credible intervals, using analytically derived density of the posterior predictive.
credible_lower <- function(x_new,mu,varmat) {
qnorm(alpha / 2, mean = x_new%*%mu, sd = sqrt(sigma^2+x_new%*%varmat%*%t(x_new)))
}

credible_upper <- function(x_new,mu,varmat) {
qnorm(1 - alpha / 2, mean = x_new%*%mu, sd = sqrt(sigma^2+x_new%*%varmat%*%t(x_new)))
}

# Calculate the credible interval bounds over the grid(drop preserves dimension of extracted vector)
lower_bounds <- sapply(1:nrow(cred_x_new_mat), function(i) credible_lower(cred_x_new_mat[i, , drop=F], miss.spec.post.mu,
miss.spec.post.varmat))
upper_bounds <- sapply(1:nrow(cred_x_new_mat), function(i) credible_upper(cred_x_new_mat[i, , drop=F], miss.spec.post.mu,
miss.spec.post.varmat))

# Plot the data points
plot(X[, 2], Y, pch = 19, col = "blue", 
     main = "90% posterior predictive credible bands for simulated data", 
     xlab = "x", ylab = "y", xlim = range(X[, 2]), ylim = range(Y, lower_bounds, upper_bounds))

polygon(c(cred_x_grid, rev(cred_x_grid)), 
        c(upper_bounds, rev(lower_bounds)), 
        col = rgb(1, 0, 0, 0.2), border = NA)

# Add smooth lines for the credible bands
lines(cred_x_grid, lower_bounds, col = "red", lwd = 2, lty = 2)  # Lower bound line
lines(cred_x_grid, upper_bounds, col = "red", lwd = 2, lty = 2)  # Upper bound line

# for the 100 data points 10 of them are outside the bands!

# Add a legend
legend("bottomright", 
       legend = c("Credible bands", "Conformal bands"), 
       col = c("red", "blue"), 
       lty = 2, lwd = 2, 
       border = NA, bty = "n")  # Remove legend box
```






