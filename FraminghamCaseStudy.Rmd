---
title: "R Notebook"
output:
  pdf_document: default
  html_notebook: default
---


```{r}
#clear global environment
rm(list=ls())
```

```{r}
#source for functions
source('/Users/Fionn/Capstone Folder/Capstone Project/AOI.classification.utils.R')
source('/Users/Fionn/Capstone Folder/Capstone Project/AOI.IS.utils.R')
```


```{r}
#read in data
data<-read.csv("/Users/Fionn/Downloads/framingham.csv")
head(data)
#check which variables have missing values
colnames(data)[colSums(is.na(data)) > 0]

```

```{r}
#for Polya-Gamma Augmentation
library("BayesLogit")
#for data cleaning
library("mice")
#for data manipulation
library("dplyr")
```


```{r}
# Convert categorical variables(that have NA's) to factors
data <- data %>%
  mutate(
    education = factor(education),
    BPMeds = factor(BPMeds)
  )

# Verify structure
str(data)
```


```{r}
imputation_methods <- make.method(data)
 # Predictive Mean Matching for numeric variables
imputation_methods[c("totChol", "BMI", "heartRate", "glucose", "cigsPerDay")] <- "pmm"  
imputation_methods["education"] <- "polr"  # ordered probit
imputation_methods["BPMeds"] <- "logreg"   # Logistic regression for binary categorical
# Run multiple imputation
imputed_data <- mice(data, method = imputation_methods, m = 5, seed = 123)

# Inspect imputed values
#print(imputed_data)
# Select one imputed dataset
filled_data <- complete(imputed_data, 1)

```

```{r}
#check no NA's remain
sum(is.na(filled_data))
```



```{r}

final.data <- one_hot_encode(filled_data, "education")
columns_to_standardize <- c(2,12,13,14,15,16,17)
for (col in columns_to_standardize) {
  final.data[[col]] <- as.numeric(scale(final.data[[col]]))
}
# Get the non-zero indices
non_zero_indices <- which(final.data[[7]] != 0)

# Extract non-zero values
non_zero_values <- final.data[[7]][non_zero_indices]

# Standardize only the non-zero values
final.data[[7]][non_zero_indices] <- (non_zero_values - mean(non_zero_values)) / sd(non_zero_values)
head(final.data)
```


```{r}
# Convert categorical variables to numeric
data_numeric <- final.data%>%
  mutate(across(where(is.character), as.factor)) %>%  # Convert character to factor
  mutate(across(where(is.factor), as.numeric)) %>%  # Convert factor to numeric
  mutate(intercept = 1)  # Add intercept before converting to matrix

# Define outcome and predictor variables
y <- data_numeric$TenYearCHD  # Outcome column
X <- data_numeric %>%
  dplyr::select(-TenYearCHD) %>%  # Predictor columns
  as.matrix()  # Convert to matrix

# Check structure
print(str(X))  # Ensure X is numeric
print(dim(X))  # Confirm correct dimensions
print(head(X))  # Preview the first few rows

```


```{r}
#set seed before generating random indices
set.seed(2003)
#hold back some rows to make predictions at
testindices <- sample(c(1:4240), size=100)
trainindices <- c(1:4240)[-testindices]
#partition test data
test.data <- X[testindices , ,drop=F]
test.resp <- y[testindices]
#partition training data
train.data <- X[trainindices , ,drop=F]
train.resp <- y[trainindices]
```




```{r}
#Generate samples of beta on training data, take prior to be N(0,1000)
beta_samples <- gibbs_sampler(y=train.resp , X=train.data, n_iter = 8000,b_prior=rep(0, 18), B_prior_inv=diag(0.01, 18))

#Preview means of beta
beta_posterior_mean <- colMeans(beta_samples)
# Print results
print(beta_posterior_mean)

```


```{r}
# quick gut check to see that estimates obtained from Polya Gamma are reasonable
  glm_fit <- glm(train.resp ~ train.data - 1, family = binomial)  #remove intercept from X
  
  # Extract MLE coefficients
  glm_coefs <- coef(glm_fit)
 glm_coefs
   #should be close to b-hat-posterior mean
```



```{r}
#Compute Model Accuracy and confusion matrix 
# Compute linear predictor (log-odds)
log_odds <- train.data %*% beta_posterior_mean  

# Convert to probabilities using the sigmoid function
predicted_probs <- 1 / (1 + exp(-log_odds))
predicted_labels <- ifelse(predicted_probs > 0.5, 1, 0)
# Compute Accuracy
accuracy <- mean(predicted_labels == train.resp)
print(paste("Model Accuracy:", round(accuracy, 3)))

# Create Confusion Matrix
table(Predicted = predicted_labels, Actual = train.resp)

```



```{r}
#may be redundant.,.,..
#find laplace approximation to the posterior
source("/Users/Fionn/Downloads/ModelEvidenceLogistic.R")
eobj <- evidence.obj( y, X, rep(0, 18) ,diag(0.01, 18) )

bhat <- eobj$newton.method()
regfisherinf <- -eobj$hessian( bhat )
postprec <- chol2inv( chol(regfisherinf)) 
gl <- glm( y~X-1, family=binomial)
# check
gl$coefficients # should be close to bhat
bhat
```



```{r}
#transpose beta for input
beta<-t(beta_samples)
#create exact Y grid
y.grid<-list(0,1)
#assign held back points to x_new
x_new <- test.data
dim(x_new)
gibbs.SE(t(beta))
```



```{r}
#get conformal sets on test data
CB.sets<-conf.sets.parallel(x_new=test.data, train.resp=train.resp, train.data=train.data, y.grid=y.grid, beta=beta, alpha=0.1)
```


```{r}
CB.sets
bayes.pred(x_new,beta,0.1)
which(bayes.pred(x_new,beta,0.1)=="{0,1}")
which(test.resp==1)
```

```{r}
coverage<-function(true.label, pred.sets){
  counter<-numeric(length(true.label))
  for (i in 1:length(true.label)){
    if (true.label[i]==1 && (pred.sets[i]=="{0,1}" || pred.sets[i]=="{1}")){
      counter[i]<-1
    }
    else if (true.label[i]==0 && (pred.sets[i]=="{0,1}" || pred.sets[i]=="{0}")){
      counter[i]<-1
    }
    else {
      counter[i]<-0
    }
  }
  return(mean(counter))
}
coverage(test.resp, CB.sets)
coverage(test.resp,bayes.pred(x_new,beta,0.1) )
```
```{r}
ave.size<-function(pred.sets){
  counter<-numeric(length(pred.sets))
  for (i in 1:length(pred.sets)){
    if(pred.sets [i]=="{}"){
      counter[i]<-0
    }
    else if (pred.sets [i]=="{0,1}"){
      counter[i]<-2
    }
    else {
      counter[i]<-1
    }
  }
  return(mean(counter))
}
ave.size(CB.sets)
ave.size(bayes.pred(x_new,beta,0.1))
```

```{r}
#compute_single_element_misclassification(predictions = CB.sets, actuals=test.resp)
compute_single_element_misclassification(predictions = bayes.pred(x_new,beta,0.2), actuals=test.resp)
```




```{r}
# Define alpha values to test
alpha_values <- seq(0.05, 0.3, by = 0.025)

# Run analysis
results <- plot_uninformative_rates(
  train.resp = train.resp,
  train.data = train.data,
  y.grid = y.grid,
  x_new = x_new,
  theta = beta,
  alpha_values = alpha_values
)

# Print results table
print(results)
```


```{r}
bayes_plot_uninformative_rates <- function(x_new, theta, 
                                   alpha_values = seq(0.01, 0.5, by = 0.05)) {
  
  # Create storage for results
  results <- data.frame(
    alpha = alpha_values,
    empty_count = 0,
    full_count = 0,
    uninformative_rate = 0
  )
  
  # For each alpha value
  for (a_idx in 1:length(alpha_values)) {
    alpha <- alpha_values[a_idx]
    cat(sprintf("Processing alpha = %.2f (%d/%d)\n", alpha, a_idx, length(alpha_values)))
    
    # Preallocate store for efficiency
    store <- vector("character", length = nrow(x_new))
    
    # Loop through each row of x_new
    for (i in 1:nrow(x_new)) {
      # Pass only the current row to bayes.pred
      store[i] <- bayes.pred(x_new = x_new[i, , drop=FALSE], theta = theta, alpha = alpha)
      
      # Optional progress indicator 
      if (i %% 50 == 0 || i == nrow(x_new)) {
        cat(sprintf("  Progress: %d/%d rows completed\r", i, nrow(x_new)))
        flush.console()
      }
    }
    
    # Count empty and full sets
    empty_count <- sum(store == "{}")
    full_count <- sum(store == "{0,1}")
    uninformative_count <- empty_count + full_count
    
    # Store results
    results$empty_count[a_idx] <- empty_count
    results$full_count[a_idx] <- full_count
    results$uninformative_rate[a_idx] <- uninformative_count/nrow(x_new)
    
    cat("\n")
  }
  
  # Plot the results
  par(mfrow=c(1,2))
  
  # Plot uninformative rate
  plot(results$alpha, results$uninformative_rate, type="o", col="blue", 
       xlab="Alpha", ylab="Uninformative Prediction Rate",
       main="Rate of Uninformative Predictions", ylim=c(0.1, 0.9))
  grid()
  
  # Plot breakdown of empty vs full sets
  barplot(t(as.matrix(results[,c("empty_count", "full_count")])), 
          beside=TRUE, names.arg=results$alpha,
          col=c("red", "green"), 
          main="Breakdown of Uninformative Predictions",
          xlab="Alpha", ylab="Count")
  legend("topright", legend=c("Full sets {0,1}"), 
         fill=c("green"))
  
  # Reset plot settings
  par(mfrow=c(1,1))
  
  return(results)
}
```


```{r}
# Define alpha values to test
alpha_values <- seq(0.05, 0.3, by = 0.025)

# Run analysis
results <- bayes_plot_uninformative_rates(
  x_new = x_new,
  theta = beta,
  alpha_values = alpha_values
)

# Print results table
print(results)
```


```{r}
bayes_plot_missclassify_rates <- function(x_new, y_true, theta, 
                                   alpha_values = seq(0.01, 0.5, by = 0.05)) {
  
  # Create storage for results
  results <- data.frame(
    alpha = alpha_values,
    empty_count = 0,
    full_count = 0,
    single_element_count = 0,
    misclassification_count = 0,
    uninformative_rate = 0,
    misclassification_rate = 0
  )
  
  # For each alpha value
  for (a_idx in 1:length(alpha_values)) {
    alpha <- alpha_values[a_idx]
    cat(sprintf("Processing alpha = %.2f (%d/%d)\n", alpha, a_idx, length(alpha_values)))
    
    # Preallocate store for efficiency
    store <- vector("character", length = nrow(x_new))
    
    # Loop through each row of x_new
    for (i in 1:nrow(x_new)) {
      # Pass only the current row to bayes.pred
      store[i] <- bayes.pred(x_new = x_new[i, , drop=FALSE], theta = theta, alpha = alpha)
      
      # Optional progress indicator 
      if (i %% 50 == 0 || i == nrow(x_new)) {
        cat(sprintf("  Progress: %d/%d rows completed\r", i, nrow(x_new)))
        flush.console()
      }
    }
    
    # Count empty and full sets
    empty_count <- sum(store == "{}")
    full_count <- sum(store == "{0,1}")
    uninformative_count <- empty_count + full_count
    
    # Count single-element predictions and misclassifications
    single_0_indices <- store == "{0}"
    single_1_indices <- store == "{1}"
    single_element_count <- sum(single_0_indices) + sum(single_1_indices)
    
    # Calculate misclassifications for single-element predictions
    misclassifications <- 0
    
    # For predictions of {0}, check if true label is 1
    misclassifications <- misclassifications + sum(single_0_indices & y_true == 1)
    
    # For predictions of {1}, check if true label is 0
    misclassifications <- misclassifications + sum(single_1_indices & y_true == 0)
    
    # Store results
    results$empty_count[a_idx] <- empty_count
    results$full_count[a_idx] <- full_count
    results$single_element_count[a_idx] <- single_element_count
    results$misclassification_count[a_idx] <- misclassifications
    results$uninformative_rate[a_idx] <- uninformative_count/nrow(x_new)
    
    # Calculate misclassification rate (only among single-element predictions)
    if (single_element_count > 0) {
      results$misclassification_rate[a_idx] <- misclassifications/single_element_count
    } else {
      results$misclassification_rate[a_idx] <- NA
    }
    
    cat("\n")
  }
  
  # Plot only the misclassification rate
  plot(results$alpha, results$misclassification_rate, type="o", col="purple", 
       xlab="Alpha", ylab="Misclassification Rate",
       main="Misclassification Rate (Single-Element Predictions)")
  grid()
  
  return(results)
}
```


```{r}
bayes_plot_missclassify_rates(x_new= x_new, y_true=test.resp, theta=beta,alpha_values = seq(0.05, 0.3, by = 0.025) )
```


```{r}
conf_plot_missclassify_rates <- function(train.resp, train.data, y.grid, x_new, y_true, theta, 
                                 alpha_values = seq(0.01, 0.5, by = 0.05)) {
  
  # Create storage for results
  results <- data.frame(
    alpha = alpha_values,
    empty_count = 0,
    full_count = 0,
    single_element_count = 0,
    misclassification_count = 0,
    uninformative_rate = 0,
    misclassification_rate = 0
  )
  
  # For each alpha value
  for (a_idx in 1:length(alpha_values)) {
    alpha <- alpha_values[a_idx]
    cat(sprintf("Processing alpha = %.2f (%d/%d)\n", alpha, a_idx, length(alpha_values)))
    
    # Preallocate store for efficiency
    store <- vector("character", length = nrow(x_new))
    
    
    # Use the parallel version to get all confidence sets at once
    store <- conf.sets.parallel(
      x_new = x_new,
      train.resp = train.resp, 
      train.data = train.data, 
      y.grid = y.grid, 
      beta = theta, 
      alpha = alpha
    )
    
      
      # Optional progress indicator 

    
    # Count empty and full sets
    empty_count <- sum(store == "{}")
    full_count <- sum(store == "{0,1}")
    uninformative_count <- empty_count + full_count
    
    # Count single-element predictions and misclassifications
    single_0_indices <- store == "{0}"
    single_1_indices <- store == "{1}"
    single_element_count <- sum(single_0_indices) + sum(single_1_indices)
    
    # Calculate misclassifications for single-element predictions
    misclassifications <- 0
    
    # For predictions of {0}, check if true label is 1
    misclassifications <- misclassifications + sum(single_0_indices & y_true == 1)
    
    # For predictions of {1}, check if true label is 0
    misclassifications <- misclassifications + sum(single_1_indices & y_true == 0)
    
    # Store results
    results$empty_count[a_idx] <- empty_count
    results$full_count[a_idx] <- full_count
    results$single_element_count[a_idx] <- single_element_count
    results$misclassification_count[a_idx] <- misclassifications
    results$uninformative_rate[a_idx] <- uninformative_count/nrow(x_new)
    
    # Calculate misclassification rate (only among single-element predictions)
    if (single_element_count > 0) {
      results$misclassification_rate[a_idx] <- misclassifications/single_element_count
    } else {
      results$misclassification_rate[a_idx] <- NA
    }
    
    cat("\n")
  }
  
  # Plot only the misclassification rate
  plot(results$alpha, results$misclassification_rate, type="o", col="purple", 
       xlab="Alpha", ylab="Misclassification Rate",
       main="Misclassification Rate (Single-Element Predictions)")
  grid()
  
  return(results)
}
```



```{r}
conf_plot_missclassify_rates(train.resp, train.data, y.grid, x_new, y_true=test.resp, theta=beta, 
                                 alpha_values = seq(0.05, 0.7, by = 0.05))
```


```{r}
compare_missclassify_rates(train.resp, train.data, y.grid, x_new, y_true=test.resp, theta=beta, 
                                 alpha_values = seq(0.05, 0.5, by = 0.05))
```

