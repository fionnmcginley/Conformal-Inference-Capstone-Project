---
title: "R Notebook"
output: html_notebook
---
Ë†

```{r}
#read in data
data<-read.csv("/Users/Fionn/Downloads/framingham.csv")
head(data)
#check which variables have missing values
colnames(data)[colSums(is.na(data)) > 0]

```

```{r}
#for Polya-Gamma Augmentation
library("BayesLogit")
#for data cleaning
library("mice")
#for data manipulation
library("dplyr")
```


```{r}
# Convert categorical variables(that have NA's) to factors
data <- data %>%
  mutate(
    education = factor(education),
    BPMeds = factor(BPMeds)
  )

# Verify structure
str(data)
```


```{r}
imputation_methods <- make.method(data)
 # Predictive Mean Matching for numeric variables
imputation_methods[c("totChol", "BMI", "heartRate", "glucose", "cigsPerDay")] <- "pmm"  
imputation_methods["education"] <- "polr"  # ordered probit
imputation_methods["BPMeds"] <- "logreg"   # Logistic regression for binary categorical
# Run multiple imputation
imputed_data <- mice(data, method = imputation_methods, m = 5, seed = 123)

# Inspect imputed values
#print(imputed_data)
# Select one imputed dataset
filled_data <- complete(imputed_data, 1)

```

```{r}
#check no NA's remain
sum(is.na(filled_data))
str(filled_data$education)
```



```{r}
#need to create dummy variables in education for logistic regression
#make 1st level of education a reference category
one_hot_encode <- function(df, cat_var) {
  df[[cat_var]] <- droplevels(factor(df[[cat_var]]))  # Ensure it's a factor
  dummies <- model.matrix(~ df[[cat_var]], data = df)[, -1, drop = FALSE]  # Remove first column (reference)
  colnames(dummies) <- sub("df\\[\\[cat_var\\]\\]", cat_var, colnames(dummies))  #  # Find the original column index
  col_index <- which(names(df) == cat_var)

  # Create a new dataframe with the dummy columns inserted at the original position
  df_new <- cbind(df[ , 1:(col_index - 1), drop = FALSE],  # Columns before categorical
                  dummies,  # One-hot encoded variables
                  df[ , (col_index + 1):ncol(df), drop = FALSE])  # Columns after

  return(df_new)
}
final.data <- one_hot_encode(filled_data, "education")
head(final.data)
```


```{r}
# Convert categorical variables to numeric
data_numeric <- final.data%>%
  mutate(across(where(is.character), as.factor)) %>%  # Convert character to factor
  mutate(across(where(is.factor), as.numeric)) %>%  # Convert factor to numeric
  mutate(intercept = 1)  # Add intercept before converting to matrix

# Define outcome and predictor variables
y <- data_numeric$TenYearCHD  # Outcome column
X <- data_numeric %>%
  dplyr::select(-TenYearCHD) %>%  # Predictor columns
  as.matrix()  # Convert to matrix

# Check structure
print(str(X))  # Ensure X is numeric
print(dim(X))  # Confirm correct dimensions
print(head(X))  # Preview the first few rows

```


```{r}
#set seed before generating random indices
set.seed(2003)
#hold back some rows to make predictions at
testindices <- sample(c(1:4240), size=100)
trainindices <- c(1:4240)[-testindices]
#partition test data
test.data <- X[testindices , ,drop=F]
test.resp <- y[testindices]
#partition training data
train.data <- X[trainindices , ,drop=F]
train.resp <- y[trainindices]
```



```{r}
# function to set up Gibbs Sampler for beta using Polya-Gamma augmentation
gibbs_sampler <- function(y, X, n_iter = 5000, b_prior = NULL, B_prior_inv = NULL) {
  
  # Data dimensions
  N <- length(y)  # Number of observations
  P <- ncol(X)    # Number of predictors (including intercept)
  
  # Set priors if not provided
  if (is.null(b_prior)) {
    b_prior <- rep(0, P)  # Mean of Gaussian prior
  }
  if (is.null(B_prior_inv)) {
    B_prior_inv <- diag(1, P)  # Precision matrix 
  }
  
  # Initialize storage for beta samples
  beta_samples <- matrix(0, nrow = n_iter, ncol = P)
  
  # Initial values
  set.seed(123)
  beta_init<-rnorm(P, mean=0, sd=0.01)
  beta <- beta_init  # Start with values close to zero
  
  # Gibbs sampling loop
  for (iter in 1:n_iter) {
    
    # Compute psi = X * beta (log-odds)
    psi <- X %*% beta
    
    # Sample omega from Polya-Gamma distribution
    omega <- rpg(N, rep(1, N), psi)  # PG(1, psi) for logistic regression
    
    # Construct diagonal matrix Omega
   #Omega <- diag(as.vector(omega), N, N)
    
    # Compute kappa
    kappa <- y - 0.5  # Equivalent to (y - n/2) since n = 1
    
    # Compute posterior covariance and mean
    # t(X) %*% (X * omega)
    A       <- t(X) %*%(X*omega)+ B_prior_inv
    R      <- chol(A)
    R.inv   <-backsolve(R, diag(rep(1,ncol(R))))
    V_omega <- R.inv%*%t(R.inv)
    m_omega <- V_omega %*% (t(X) %*% kappa + B_prior_inv %*% b_prior)
    
    # Generate N(0,1) deviates in length of m_omega
    z_i <- rnorm(length(m_omega), 0, 1)
    #apply transformation so beta~MVN(m_omega,V_omega)
    beta <- (R.inv%*%z_i)+m_omega
    
    # Store the sample
    beta_samples[iter, ] <- beta
  }
  
  return(beta_samples)
}
```


```{r}
#Generate samples of beta on training data, take prior to be N(0,1000)
beta_samples <- gibbs_sampler(y=train.resp , X=train.data, n_iter = 5000,b_prior=rep(0, 18), B_prior_inv=diag(0.001, 18))

#Preview means of beta
beta_posterior_mean <- colMeans(beta_samples)
# Print results
print(beta_posterior_mean)

```


```{r}
# quick gut check to see that estimates obtained from Polya Gamma are reasonable
  glm_fit <- glm(train.resp ~ train.data - 1, family = binomial)  #remove intercept from X
  
  # Extract MLE coefficients
  glm_coefs <- coef(glm_fit)
 glm_coefs
   #should be close to b-hat-posterior mean
```



```{r}
#Compute Model Accuracy and confusion matrix 
# Compute linear predictor (log-odds)
log_odds <- train.data %*% beta_posterior_mean  

# Convert to probabilities using the sigmoid function
predicted_probs <- 1 / (1 + exp(-log_odds))
predicted_labels <- ifelse(predicted_probs > 0.5, 1, 0)
# Compute Accuracy
accuracy <- mean(predicted_labels == train.resp)
print(paste("Model Accuracy:", round(accuracy, 3)))

# Create Confusion Matrix
table(Predicted = predicted_labels, Actual = train.resp)

```


```{r}
#may be redundant.,.,..
#find laplace approximation to the posterior
source("/Users/Fionn/Downloads/ModelEvidenceLogistic.R")
eobj <- evidence.obj( y, X, rep(0, 18) ,diag(0.01, 18) )

bhat <- eobj$newton.method()
regfisherinf <- -eobj$hessian( bhat )
postprec <- chol2inv( chol(regfisherinf)) 
gl <- glm( y~X-1, family=binomial)
# check
gl$coefficients # should be close to bhat
bhat
```



```{r}
#function to do AOI importance sampling to estimate the posterior predictive
logistic_posterior_predictive <- function(Y, X, y, x_new, theta) {
  # Y: binary outcome (0,1) for existing observation
  # X: design vector for existing observation (including intercept)
  # y: reference outcome value (0,1) for computing weights
  # x_new: design vector for new point (for computing weights)
  # theta: MCMC parameter samples (each column is a sample)
 
  n_samples <- ncol(theta)
  
  # Define dispatch table with just two cases (y=0 and y=1)
  dispatch_table <- list(
    # Case: y is 1
    "one" = function() {
      # Compute logistic probabilities for y=1 at x_new
      w <- sapply(1:n_samples, function(t) {
        1 / (1 + exp(-x_new %*% theta[, t]))
      })
      
      # Normalize weights
      normalized_weights <- w/sum(w)
      
      # Compute weighted likelihoods
      weighted_likelihoods <- sapply(1:n_samples, function(t) {
        normalized_weights[t] * (1 / (1 + exp(-X %*% theta[, t])))
      })
      
      if(Y == 1) {
        return(sum(weighted_likelihoods))
      } else { # Y == 0
        return(1 - sum(weighted_likelihoods))
      }
    },
    
    # Case: y is 0
    "zero" = function() {
      # Compute logistic probabilities for y=1 at x_new
      w <- sapply(1:n_samples, function(t) {
        1 / (1 + exp(-x_new %*% theta[, t]))
      })
      
      # Normalize weights for y=0
      normalized_weights <- (1-w)/sum(1-w)
      
      # Compute weighted likelihoods
      weighted_likelihoods <- sapply(1:n_samples, function(t) {
        normalized_weights[t] * (1 / (1 + exp(X %*% theta[, t])))
      })
      
      if(Y == 1) {
        return(sum(weighted_likelihoods))
      } else { # Y == 0
        return(1 - sum(weighted_likelihoods))
      }
    }
  )
  
  # Execute appropriate function based on y value
  if(y == 1) {
    return(dispatch_table[["one"]]())
  } else { # y == 0
    return(dispatch_table[["zero"]]())
  }
}
```


```{r}
X<-as.matrix(X)
dim(X)
dim(X[1, ,drop=F])
outcome<-as.vector(y)
beta<-t(beta_samples)
beta[,1]
logistic_posterior_predictive(Y=outcome[5], X=X[4, ,drop=F], y=c(0,1), x_new=x_new[1, ,drop=F], theta=beta)
```


```{r}
# Function to generate test points with manually specified variable types and last column as intercept
generate_test_points_with_noise <- function(X, 
                                           binary_indices, 
                                           numeric_indices,
                                           n_samples = 1000, 
                                           numeric_noise = 0.5, 
                                           binary_flip_prob = 0.2, 
                                           seed = 123) {
  set.seed(seed)
  
  # Get dimensions
  n <- nrow(X)
  p <- ncol(X)
  intercept_col <- p  # Last column is intercept
  
  # Sample from original data
  sampled_indices <- sample(1:n, n_samples, replace = TRUE)
  test_points <- X[sampled_indices, ]
  
  # Make sure intercept column is excluded from noise addition
  binary_indices <- binary_indices[binary_indices != intercept_col]
  numeric_indices <- numeric_indices[numeric_indices != intercept_col]
  
  # Add noise to numeric variables
  for(i in numeric_indices) {
    # Calculate standard deviation of the column
    col_sd <- sd(X[, i])
    
    # Add small Gaussian noise
    test_points[, i] <- round((test_points[, i] + rnorm(n_samples, 0, numeric_noise * col_sd)))
  }
  
  # Add noise to binary variables by randomly flipping values
  for(i in binary_indices) {
    # Generate random values between 0 and 1
    flip <- runif(n_samples) < binary_flip_prob
    
    # Flip binary values where flip is TRUE
    test_points[flip, i] <- 1 - test_points[flip, i]
  }
  
  return(test_points)
}
```


```{r}
#randomly samples with replacement from dataset, adds Gaussian noise to numeric variables(wrt the variance along a column), and flips binary variables with a pre-specified probability(leave education and cigs/day out for simplicity)
x_new<-generate_test_points_with_noise(X,c(1,8,9,10,11), c(2,12,13,14,15,16,17),n_samples=50,seed=2003)
dim(x_new[1, ,drop=F])
```s

```{r}
#create exact Y grid
y.grid<-list(0,1)
length(y.grid)
#assign held back points to x_new
x_new <- test.data
```


```{r}
x_new <- test.data
x_new[1,]
# Preallocate store for efficiency
store <- vector("character", length = nrow(x_new))

# Loop through each row of x_new
for (i in 1:nrow(x_new)) {
  store[i] <- full_conformal_prediction(
    Y = y[rand.indices], 
    X = X[rand.indices, , drop = FALSE], 
    y_grid = y.grid, 
    x_new = x_new[i, , drop = FALSE],  # Iterate through rows
    theta = beta, 
    alpha = 0.2
  )

  # Progress indicator every 10 iterations
  if (i %% 10 == 0 || i == nrow(x_new)) {
    cat(sprintf("Progress: %d/%d iterations completed.\n", i, nrow(x_new)))
    flush.console()  # Ensure output appears in real-time
  }
}

```


```{r}
store
```


```{r}
full_conformal_prediction <- function(Y, X, y_grid, x_new, theta, alpha) {
  # Track which values are accepted
  accepted_0 <- FALSE
  accepted_1 <- FALSE
  
  for (l in 1:length(y_grid)) {
    # Conformity scores on dataset
    sig_1_to_n <- sapply(1:length(Y), function(i) 
      logistic_posterior_predictive(Y = Y[i], X = X[i, , drop = FALSE], y = y_grid[[l]], x_new = x_new, theta = theta)
    )
    # Conformity score on test point
    sig_n_plus_one <- logistic_posterior_predictive(Y = y_grid[[l]], X = x_new, y = y_grid[[l]], x_new = x_new, theta = theta)
    # Adjusted quantile calculation
    n <- length(Y)
    pi <- (length(which(sig_1_to_n <= sig_n_plus_one)) + 1) / (n + 1)
    # Reject points if pi <= alpha
    if (pi > alpha) {
      # Mark the value as accepted
      if (y_grid[[l]] == 0) {
        accepted_0 <- TRUE
      } else if (y_grid[[l]] == 1) {
        accepted_1 <- TRUE
      }
    }
  }
  
  # Format the output string based on which values were accepted
  if (accepted_0 && accepted_1) {
    return("{0,1}")
  } else if (accepted_0) {
    return("{0}")
  } else if (accepted_1) {
    return("{1}")
  } else {
    return("{}")
  }
}
```


```{r}

```

