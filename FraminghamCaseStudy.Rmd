---
title: "R Notebook"
output:
  pdf_document: default
  html_notebook: default
---


```{r}
#clear global environment
rm(list=ls())
```

```{r}
#source for functions
source('/Users/Fionn/Capstone Folder/Capstone Project/AOI.classification.utils.R')
```


```{r}
#read in data
data<-read.csv("/Users/Fionn/Downloads/framingham.csv")
head(data)
#check which variables have missing values
colnames(data)[colSums(is.na(data)) > 0]

```

```{r}
#for Polya-Gamma Augmentation
library("BayesLogit")
#for data cleaning
library("mice")
#for data manipulation
library("dplyr")
```


```{r}
# Convert categorical variables(that have NA's) to factors
data <- data %>%
  mutate(
    education = factor(education),
    BPMeds = factor(BPMeds)
  )

# Verify structure
str(data)
```


```{r}
imputation_methods <- make.method(data)
 # Predictive Mean Matching for numeric variables
imputation_methods[c("totChol", "BMI", "heartRate", "glucose", "cigsPerDay")] <- "pmm"  
imputation_methods["education"] <- "polr"  # ordered probit
imputation_methods["BPMeds"] <- "logreg"   # Logistic regression for binary categorical
# Run multiple imputation
imputed_data <- mice(data, method = imputation_methods, m = 5, seed = 123)

# Inspect imputed values
#print(imputed_data)
# Select one imputed dataset
filled_data <- complete(imputed_data, 1)

```

```{r}
#check no NA's remain
sum(is.na(filled_data))
```



```{r}

final.data <- one_hot_encode(filled_data, "education")
columns_to_standardize <- c(2,12,13,14,15,16,17)
for (col in columns_to_standardize) {
  final.data[[col]] <- as.numeric(scale(final.data[[col]]))
}
# Get the non-zero indices
non_zero_indices <- which(final.data[[7]] != 0)

# Extract non-zero values
non_zero_values <- final.data[[7]][non_zero_indices]

# Standardize only the non-zero values
final.data[[7]][non_zero_indices] <- (non_zero_values - mean(non_zero_values)) / sd(non_zero_values)
head(final.data)
```


```{r}
# Convert categorical variables to numeric
data_numeric <- final.data%>%
  mutate(across(where(is.character), as.factor)) %>%  # Convert character to factor
  mutate(across(where(is.factor), as.numeric)) %>%  # Convert factor to numeric
  mutate(intercept = 1)  # Add intercept before converting to matrix

# Define outcome and predictor variables
y <- data_numeric$TenYearCHD  # Outcome column
X <- data_numeric %>%
  dplyr::select(-TenYearCHD) %>%  # Predictor columns
  as.matrix()  # Convert to matrix

# Check structure
print(str(X))  # Ensure X is numeric
print(dim(X))  # Confirm correct dimensions
print(head(X))  # Preview the first few rows

```
```{r}
#create multiple splits for checking
splits<-create.splits(X, y, num_splits = 50, test_size = 100, seed = 2003)
```







```{r}
#Generate samples of beta on training data, take prior to be N(0,1000)
beta_samples<-gibbs_sampler(y=splits$train_resp[[1]] , X=splits$train_data[[1]], n_iter = 4000,b_prior=rep(0, 18), B_prior_inv=diag(0.01, 18))

#Preview means of beta
beta_posterior_mean <- colMeans(beta_samples)
# Print results
print(beta_posterior_mean)

```


```{r}
# quick gut check to see that estimates obtained from Polya Gamma are reasonable
  glm_fit <- glm(splits$train_resp[[1]] ~ splits$train_data[[1]] - 1, family = binomial)  #remove intercept from X
  
  # Extract MLE coefficients
  glm_coefs <- coef(glm_fit)
 glm_coefs
   #should be close to b-hat-posterior mean
```



```{r}
#Compute Model Accuracy and confusion matrix 
# Compute linear predictor (log-odds)
log_odds <- splits$train_data[[1]] %*% beta_posterior_mean  

# Convert to probabilities using the sigmoid function
predicted_probs <- 1 / (1 + exp(-log_odds))
predicted_labels <- ifelse(predicted_probs > 0.5, 1, 0)
# Compute Accuracy
accuracy <- mean(predicted_labels == splits$train_resp[[1]])
print(paste("Model Accuracy:", round(accuracy, 3)))

# Create Confusion Matrix
table(Predicted = predicted_labels, Actual = splits$train_resp[[1]])

```



```{r}
#may be redundant.,.,..
#find laplace approximation to the posterior
source("/Users/Fionn/Downloads/ModelEvidenceLogistic.R")
eobj <- evidence.obj( y, X, rep(0, 18) ,diag(0.01, 18) )

bhat <- eobj$newton.method()
regfisherinf <- -eobj$hessian( bhat )
postprec <- chol2inv( chol(regfisherinf)) 
gl <- glm( y~X-1, family=binomial)
# check
gl$coefficients # should be close to bhat
bhat
```


```{r}
#test coverage over 20 repeats
# Create cluster once at the beginning
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

# Pre-export common functions
clusterExport(cl, c("logistic_posterior_predictive_vec", "full_conformal_classify"), 
              envir = environment())

# Prepare to collect coverage results
coverage_results <- numeric(length(splits$train_resp))

# Start timing
start_time <- Sys.time()

# Process 20 splits
for (i in 1:length(splits$train_resp)) {
  cat("Processing split", i, "at", format(Sys.time(), "%H:%M:%S"), "\n")
  beta.samples<-gibbs_sampler(y=splits$train_resp[[i]] , X=splits$train_data[[i]], n_iter = 4000,b_prior=rep(0, 18), B_prior_inv=diag(0.01, 18))
  # Use the parallel implementation with the shared cluster
  CB.sets <- conf.sets.parallel.v2(
    x_new = splits$test_data[[i]], 
    train.resp = splits$train_resp[[i]], 
    train.data = splits$train_data[[i]], 
    y.grid = list(0,1), 
    beta = t(beta.samples), 
    alpha = 0.2 )
  
  # Calculate coverage for this split
  coverage_results[i] <- coverage(splits$test_resp[[i]], CB.sets)
  
  # Report progress
  current_time <- Sys.time()
  elapsed <- difftime(current_time, start_time, units = "mins")
  cat("Completed split", i, "- Elapsed time:", round(elapsed, 2), "minutes\n")
}

# Stop the cluster when completely done
stopCluster(cl)

# Calculate summary statistics
mean_coverage <- mean(coverage_results)
sd_coverage <- sd(coverage_results)
cat("\nMean coverage:", mean_coverage, "\n")
cat("Standard deviation:", sd_coverage, "\n")
```
```{r}
coverage_results
```


```{r}

#test coverage for 50 repeats on Bayes
# Prepare to collect coverage results
coverage_results <- numeric(length(splits$train_resp))

start_time <- Sys.time()
# Process all 20 splits
for (i in 1:length(splits$train_resp)) {
    cat("Processing split", i, "at", format(Sys.time(), "%H:%M:%S"), "\n")
  beta.samples<-gibbs_sampler(y=splits$train_resp[[i]] , X=splits$train_data[[i]], n_iter = 4000,b_prior=rep(0, 18), B_prior_inv=diag(0.01, 18))
  
  # Use the parallel implementation with the shared cluster
  B.sets <-bayes.pred(splits$test_data[[i]],t(beta.samples),0.2) 
  
  # Calculate coverage for this split
  coverage_results[i] <- coverage(splits$test_resp[[i]], B.sets)
    # Report progress
  current_time <- Sys.time()
  elapsed <- difftime(current_time, start_time, units = "mins")
  cat("Completed split", i, "- Elapsed time:", round(elapsed, 2), "minutes\n")
}


# Calculate summary statistics
mean_coverage <- mean(coverage_results)
sd_coverage <- sd(coverage_results)
cat("\nMean coverage:", mean_coverage, "\n")
cat("Standard deviation:", sd_coverage, "\n")
```



```{r}
library(doParallel)
library(foreach)
#get conformal sets on test data
beta<-gibbs_sampler(y=splits$train_resp[[2]] , X=splits$train_data[[2]], n_iter = 4000,b_prior=rep(0, 18), B_prior_inv=diag(0.01, 18))

CB.sets1<-conf.sets.parallel.v2(x_new=splits$test_data[[2]], train.resp=splits$train_resp[[2]], train.data=splits$train_data[[2]], y.grid=list(0,1), beta=t(beta), alpha=0.15)
```

```{r}
library(doParallel)
library(foreach)
#get conformal sets on test data
beta<-gibbs_sampler(y=splits$train_resp[[1]] , X=splits$train_data[[1]], n_iter = 4000,b_prior=rep(0, 18), B_prior_inv=diag(0.01, 18))

CB.sets2<-conf.sets.parallel(x_new=splits$test_data[[1]], train.resp=splits$train_resp[[1]], train.data=splits$train_data[[1]], y.grid=list(0,1), beta=t(beta), alpha=0.5)
```


```{r}
CB.sets2
bayes.pred(splits$test_data[[2]],t(beta),0.5)
which(splits$test_resp[[1]]==1)
```

```{r}
splits$train_resp[[1]]
```

```{r}
coverage<-function(true.label, pred.sets){
  counter<-numeric(length(true.label))
  for (i in 1:length(true.label)){
    if (true.label[i]==1 && (pred.sets[i]=="{0,1}" || pred.sets[i]=="{1}")){
      counter[i]<-1
    }
    else if (true.label[i]==0 && (pred.sets[i]=="{0,1}" || pred.sets[i]=="{0}")){
      counter[i]<-1
    }
    else {
      counter[i]<-0
    }
  }
  return(mean(counter))
}
coverage(splits$test_resp[[1]], CB.sets2)
coverage(splits$test_resp[[3]],bayes.pred(splits$test_data[[3]],t(beta),0.2) )
```
```{r}
ave.size<-function(pred.sets){
  counter<-numeric(length(pred.sets))
  for (i in 1:length(pred.sets)){
    if(pred.sets [i]=="{}"){
      counter[i]<-0
    }
    else if (pred.sets [i]=="{0,1}"){
      counter[i]<-2
    }
    else {
      counter[i]<-1
    }
  }
  return(mean(counter))
}
ave.size(CB.sets)
ave.size(bayes.pred(x_new,beta,0.1))
```

```{r}
#compute_single_element_misclassification(predictions = CB.sets, actuals=test.resp)
compute_single_element_misclassification(predictions = bayes.pred(x_new,beta,0.1), actuals=test.resp)
compute_single_element_misclassification(predictions =CB.sets, actuals=test.resp)
```




```{r}
# Define alpha values to test
alpha_values <-seq(0.05, 0.5, by = 0.05)
beta<-gibbs_sampler(y=splits$train_resp[[3]] , X=splits$train_data[[3]], n_iter = 4000,b_prior=rep(0, 18), B_prior_inv=diag(0.01, 18))
# Run analysis
results <- plot_uninformative_rates(
  train.resp = splits$train_resp[[3]],
  train.data = splits$train_data[[3]],
  y.grid = list(0,1),
  x_new = splits$test_data[[3]],
  theta = t(beta),
  alpha_values = alpha_values
)

# Print results table
print(results)
```

```{r}
print(results)
```


```{r}
bayes_plot_uninformative_rates <- function(x_new, theta, 
                                   alpha_values = seq(0.01, 0.5, by = 0.05)) {
  
  # Create storage for results
  results <- data.frame(
    alpha = alpha_values,
    empty_count = 0,
    full_count = 0,
    uninformative_rate = 0
  )
  
  # For each alpha value
  for (a_idx in 1:length(alpha_values)) {
    alpha <- alpha_values[a_idx]
    cat(sprintf("Processing alpha = %.2f (%d/%d)\n", alpha, a_idx, length(alpha_values)))
    
    # Preallocate store for efficiency
    store <- vector("character", length = nrow(x_new))
    
    # Loop through each row of x_new
    for (i in 1:nrow(x_new)) {
      # Pass only the current row to bayes.pred
      store[i] <- bayes.pred(x_new = x_new[i, , drop=FALSE], theta = theta, alpha = alpha)
      
      # Optional progress indicator 
      if (i %% 50 == 0 || i == nrow(x_new)) {
        cat(sprintf("  Progress: %d/%d rows completed\r", i, nrow(x_new)))
        flush.console()
      }
    }
    
    # Count empty and full sets
    empty_count <- sum(store == "{}")
    full_count <- sum(store == "{0,1}")
    uninformative_count <- empty_count + full_count
    
    # Store results
    results$empty_count[a_idx] <- empty_count
    results$full_count[a_idx] <- full_count
    results$uninformative_rate[a_idx] <- uninformative_count/nrow(x_new)
    
    cat("\n")
  }
  
  # Plot the results
  par(mfrow=c(1,2))
  
  # Plot uninformative rate
  plot(results$alpha, results$uninformative_rate, type="o", col="blue", 
       xlab="Alpha", ylab="Uninformative Prediction Rate",
        ylim=c(0, 1))
  grid()
  
  # Plot breakdown of empty vs full sets
  barplot(t(as.matrix(results[,c("empty_count", "full_count")])), 
          beside=TRUE, names.arg=results$alpha,
          col=c("red", "green"), 
          
          xlab="Alpha", ylab="Count")
  legend("topright", legend=c("Full sets {0,1}"), 
         fill=c("green"))
  
  # Reset plot settings
  par(mfrow=c(1,1))
  
  return(results)
}
```


```{r}
beta<-gibbs_sampler(y=splits$train_resp[[2]] , X=splits$train_data[[2]], n_iter = 4000,b_prior=rep(0, 18), B_prior_inv=diag(0.01, 18))
# Define alpha values to test
alpha_values <- seq(0.05, 0.5, by = 0.05)

# Run analysis
results <- bayes_plot_uninformative_rates(
  x_new = splits$test_data[[2]],
  theta = t(beta),
  alpha_values = alpha_values
)

# Print results table
print(results)
```


```{r}
bayes_plot_missclassify_rates <- function(x_new, y_true, theta, 
                                   alpha_values = seq(0.01, 0.5, by = 0.05)) {
  
  # Create storage for results
  results <- data.frame(
    alpha = alpha_values,
    empty_count = 0,
    full_count = 0,
    single_element_count = 0,
    misclassification_count = 0,
    uninformative_rate = 0,
    misclassification_rate = 0
  )
  
  # For each alpha value
  for (a_idx in 1:length(alpha_values)) {
    alpha <- alpha_values[a_idx]
    cat(sprintf("Processing alpha = %.2f (%d/%d)\n", alpha, a_idx, length(alpha_values)))
    
    # Preallocate store for efficiency
    store <- vector("character", length = nrow(x_new))
    
    # Loop through each row of x_new
    for (i in 1:nrow(x_new)) {
      # Pass only the current row to bayes.pred
      store[i] <- bayes.pred(x_new = x_new[i, , drop=FALSE], theta = theta, alpha = alpha)
      
      # Optional progress indicator 
      if (i %% 50 == 0 || i == nrow(x_new)) {
        cat(sprintf("  Progress: %d/%d rows completed\r", i, nrow(x_new)))
        flush.console()
      }
    }
    
    # Count empty and full sets
    empty_count <- sum(store == "{}")
    full_count <- sum(store == "{0,1}")
    uninformative_count <- empty_count + full_count
    
    # Count single-element predictions and misclassifications
    single_0_indices <- store == "{0}"
    single_1_indices <- store == "{1}"
    single_element_count <- sum(single_0_indices) + sum(single_1_indices)
    
    # Calculate misclassifications for single-element predictions
    misclassifications <- 0
    
    # For predictions of {0}, check if true label is 1
    misclassifications <- misclassifications + sum(single_0_indices & y_true == 1)
    
    # For predictions of {1}, check if true label is 0
    misclassifications <- misclassifications + sum(single_1_indices & y_true == 0)
    
    # Store results
    results$empty_count[a_idx] <- empty_count
    results$full_count[a_idx] <- full_count
    results$single_element_count[a_idx] <- single_element_count
    results$misclassification_count[a_idx] <- misclassifications
    results$uninformative_rate[a_idx] <- uninformative_count/nrow(x_new)
    
    # Calculate misclassification rate (only among single-element predictions)
    if (single_element_count > 0) {
      results$misclassification_rate[a_idx] <- misclassifications/single_element_count
    } else {
      results$misclassification_rate[a_idx] <- NA
    }
    
    cat("\n")
  }
  
  # Plot only the misclassification rate
  plot(results$alpha, results$misclassification_rate, type="o", col="purple", 
       xlab="Alpha", ylab="Misclassification Rate",
       main="Misclassification Rate (Single-Element Predictions)")
  grid()
  
  return(results)
}
```







```{r}
beta<-gibbs_sampler(y=splits$train_resp[[2]] , X=splits$train_data[[2]], n_iter = 4000,b_prior=rep(0, 18), B_prior_inv=diag(0.01, 18))
compare_missclassify_rates(splits$train_resp[[2]],splits$train_data[[2]], y.grid=list(0,1), x_new=splits$test_data[[2]], y_true=splits$test_resp[[2]], theta=t(beta), 
                                 alpha_values = seq(0.05, 0.5, by = 0.05))
```

```{r}
compare_class_prediction_counts(splits$train_resp[[2]], splits$train_data[[2]], y.grid=list(0,1), x_new=splits$test_data[[2]], theta=t(beta), 
                                      alpha_values = seq(0.01, 0.5, by = 0.05))
```

